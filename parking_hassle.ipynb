{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264dce70",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d18854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a93b8",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "The PKLot dataset contains 12,416 images of parking lots extracted from surveilance camera frames. There are images on sunny, cloudy, and rainy days and the parking spaces are labeled as occupied or empty. We have converted the original annotations to a variety of standard object detection formats by enclosing a bounding box around the original dataset's rotated rectangle annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cce699de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the PKLot dataset\n",
    "data_dir = 'train/'\n",
    "annotations_file = 'train/_annotations.csv'\n",
    "test_data_dir = 'test/'\n",
    "test_annotations_file = 'test/_annotations.csv'\n",
    "image_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a51ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(annotations_file)\n",
    "test_annotations = pd.read_csv(test_annotations_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5552cb5e",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5667540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating arrays for images and labels depicting if the space is occupied or empty\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for image_file in annotations['filename'].unique():\n",
    "    image_path = os.path.join(data_dir, image_file)\n",
    "    image = Image.open(image_path).resize(image_size)\n",
    "    images.append(np.array(image))\n",
    "    \n",
    "    # Check if any annotation for 'space-occupied' exists in the image annotations\n",
    "    if annotations[(annotations['filename'] == image_file) & (annotations['class'] == 'space-occupied')].shape[0] > 0:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "# Converting the arrays into numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b3783dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating arrays for images and labels depicting if the space is occupied or empty\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "for image_file in test_annotations['filename'].unique():\n",
    "    image_path = os.path.join(test_data_dir, image_file)\n",
    "    image = Image.open(image_path).resize(image_size)\n",
    "    test_images.append(np.array(image))\n",
    "    \n",
    "    # Check if any annotation for 'space-occupied' exists in the image annotations\n",
    "    if test_annotations[(test_annotations['filename'] == image_file) & (test_annotations['class'] == 'space-occupied')].shape[0] > 0:\n",
    "        test_labels.append(1)\n",
    "    else:\n",
    "        test_labels.append(0)\n",
    "\n",
    "# Converting the arrays into numpy arrays\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57678e4",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5d0956",
   "metadata": {},
   "source": [
    "#### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cab359da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification output\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333ca2e",
   "metadata": {},
   "source": [
    "#### Training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76187bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "266/266 [==============================] - 462s 2s/step - loss: 10.5101 - accuracy: 0.9076 - val_loss: 0.0645 - val_accuracy: 0.9762\n",
      "Epoch 2/10\n",
      "266/266 [==============================] - 465s 2s/step - loss: 0.5771 - accuracy: 0.8576 - val_loss: 0.2069 - val_accuracy: 0.8939\n",
      "Epoch 3/10\n",
      "266/266 [==============================] - 470s 2s/step - loss: 0.2787 - accuracy: 0.8713 - val_loss: 0.3919 - val_accuracy: 0.8199\n",
      "Epoch 4/10\n",
      "266/266 [==============================] - 467s 2s/step - loss: 0.2805 - accuracy: 0.8667 - val_loss: 0.2256 - val_accuracy: 0.8791\n",
      "Epoch 5/10\n",
      "266/266 [==============================] - 467s 2s/step - loss: 0.1552 - accuracy: 0.9197 - val_loss: 0.0811 - val_accuracy: 0.9720\n",
      "Epoch 6/10\n",
      "266/266 [==============================] - 466s 2s/step - loss: 0.2454 - accuracy: 0.8740 - val_loss: 0.2659 - val_accuracy: 0.8914\n",
      "Epoch 7/10\n",
      "266/266 [==============================] - 466s 2s/step - loss: 0.2336 - accuracy: 0.8866 - val_loss: 0.0947 - val_accuracy: 0.9646\n",
      "Epoch 8/10\n",
      "266/266 [==============================] - 466s 2s/step - loss: 0.1112 - accuracy: 0.9485 - val_loss: 0.0519 - val_accuracy: 0.9893\n",
      "Epoch 9/10\n",
      "266/266 [==============================] - 468s 2s/step - loss: 0.0475 - accuracy: 0.9889 - val_loss: 0.0237 - val_accuracy: 0.9975\n",
      "Epoch 10/10\n",
      "266/266 [==============================] - 466s 2s/step - loss: 0.0235 - accuracy: 0.9952 - val_loss: 0.0261 - val_accuracy: 0.9959\n",
      "38/38 [==============================] - 13s 332ms/step\n",
      "Validation Accuracy: 1.00\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Empty       0.98      1.00      0.99       225\n",
      "    Occupied       1.00      0.99      1.00       991\n",
      "\n",
      "    accuracy                           1.00      1216\n",
      "   macro avg       0.99      1.00      0.99      1216\n",
      "weighted avg       1.00      1.00      1.00      1216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(images, labels, epochs=10, batch_size=32, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(test_images)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "val_accuracy = accuracy_score(test_labels, y_pred_binary)\n",
    "val_classification_rep = classification_report(test_labels, y_pred_binary, target_names=['Empty', 'Occupied'])\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy:.2f}')\n",
    "print('Validation Classification Report:\\n', val_classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116f62a7",
   "metadata": {},
   "source": [
    "We have near perfect results for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac8a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
